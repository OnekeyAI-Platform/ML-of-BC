{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ddc852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import display\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "from onekey_algo import OnekeyDS as okds\n",
    "from onekey_algo import get_param_in_cwd\n",
    "\n",
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('results', exist_ok=True)\n",
    "os.makedirs('features', exist_ok=True)\n",
    "\n",
    "\n",
    "task_type = 'metastasis'\n",
    "\n",
    "group_info = get_param_in_cwd('dataset_column') or 'group'\n",
    "labelf = r'E:/20230904-LiuHui/metastasis.csv'\n",
    "\n",
    "labels = [get_param_in_cwd('task_column') or 'label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4cbe42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import onekey_algo.custom.components as okcomp\n",
    "from onekey_algo import OnekeyDS as okds\n",
    "from onekey_algo.custom.utils import print_join_info\n",
    "from onekey_algo import get_param_in_cwd\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.makedirs('img', exist_ok=True)\n",
    "os.makedirs('features', exist_ok=True)\n",
    "\n",
    "def mapID(x):\n",
    "    items = x[:-2].split('-')\n",
    "    return f\"20{items[0]}-{int(items[1][:5]):05d}\"\n",
    "\n",
    "prob_histo = pd.read_csv('features/path_prob_histogram.csv')\n",
    "prob_tfidf = pd.read_csv('features/path_prob_tfidf.csv')\n",
    "prob = pd.merge(prob_histo, prob_tfidf, on='ID', how='inner', suffixes=['_histo', '_tfidf'])\n",
    "prob['ID'] = prob['ID'].astype(str)\n",
    "\n",
    "pred_histo = pd.read_csv('features/path_pred_histogram.csv')\n",
    "pred_tfidf = pd.read_csv('features/path_pred_tfidf.csv')\n",
    "pred = pd.merge(pred_histo, pred_tfidf, on='ID', how='inner', suffixes=['_histo', '_tfidf'])\n",
    "pred['ID'] = pred['ID'].astype(str)\n",
    "\n",
    "features = pd.merge(prob, pred, on='ID', how='inner')\n",
    "features['ID'] = features['ID'].map(mapID)\n",
    "features.to_csv('features/path_features.csv', index=False, header=True)\n",
    "labels = ['label']\n",
    "featrues_not_use = ['ID']\n",
    "\n",
    "label_data = pd.read_csv(labelf, header=0)\n",
    "label_data['ID'] = label_data['ID'].map(lambda x: x.replace('.nii.gz', ''))\n",
    "combined_data = pd.merge(features, label_data, left_on='ID', right_on='ID', how='inner')\n",
    "combined_data = pd.merge(combined_data, pd.read_csv(get_param_in_cwd('label_file'))[['ID', 'group']], on='ID', how='inner')\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.utils import print_join_info\n",
    "\n",
    "ids = combined_data['ID']\n",
    "combined_data = combined_data.drop(['ID'], axis=1)\n",
    "print(combined_data[labels].value_counts())\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5c306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fe3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import normalize_df\n",
    "# data = normalize_df(combined_data, not_norm=labels, group=group_info)\n",
    "# data = data.dropna(axis=1)\n",
    "data = combined_data\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0e44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from onekey_algo.custom.components.stats import clinic_stats\n",
    "\n",
    "stats = clinic_stats(data[data[group_info] == 'train'], stats_columns=list(data.columns[0:-2]), label_column=labels[0], \n",
    "                     continuous_columns=list(data.columns[0:-2]))\n",
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aaf8a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def map2float(x):\n",
    "    try:\n",
    "        return float(str(x)[1:])\n",
    "    except:\n",
    "        return 1\n",
    "\n",
    "stats[['pvalue']] = stats[['pvalue']].applymap(map2float)\n",
    "stats[['group']] = stats[['feature_name']].applymap(lambda x: 'BoW' if '-' in x else 'PLH')\n",
    "stats = stats[['feature_name', 'pvalue', 'group']]\n",
    "g = sns.catplot(x=\"group\", y=\"pvalue\", data=stats, kind=\"violin\")\n",
    "g.fig.set_size_inches(15,10)\n",
    "sns.stripplot(x=\"group\", y=\"pvalue\", data=stats, ax=g.ax, color='black')\n",
    "plt.savefig(f'img/{task_type}_feature_stats.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab2864",
   "metadata": {},
   "outputs": [],
   "source": [
    "pvalue = 0.05\n",
    "sel_feature = list(stats[stats['pvalue'] < pvalue]['feature_name']) + labels + [group_info]\n",
    "data = data#[sel_feature]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979a9aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pearson_corr = data[data[group_info] == 'train'][[c for c in data.columns if c not in labels]].corr('pearson')\n",
    "# kendall_corr = data[[c for c in data.columns if c not in labels]].corr('kendall')\n",
    "# spearman_corr = data[[c for c in data.columns if c not in labels]].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01558bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from onekey_algo.custom.components.comp1 import draw_matrix\n",
    "# plt.figure(figsize=(50.0, 40.0))\n",
    "\n",
    "# draw_matrix(pearson_corr, annot=True, cmap='YlGnBu', cbar=False)\n",
    "# plt.savefig(f'img/Rad_feature_corr.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd6297e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pp = sns.clustermap(pearson_corr, linewidths=.5, figsize=(50.0, 40.0), cmap='YlGnBu')\n",
    "# plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)\n",
    "# plt.savefig(f'img/Rad_feature_cluster.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe3df7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import select_feature, select_feature_mrmr\n",
    "sel_feature = select_feature(pearson_corr, threshold=0.9, topn=10, verbose=False)\n",
    "# sel_feature = select_feature_mrmr(data[sel_feature + labels], num_features=16)\n",
    "fids = features['ID']\n",
    "features = features[sel_feature]\n",
    "sel_feature = sel_feature + labels + [group_info]\n",
    "sel_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e09b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sel_data = data[sel_feature]\n",
    "sel_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f66162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import analysis_features\n",
    "analysis_features(data[sel_feature[:-2]], data[labels[0]], prefix='rad_mil_', save_dir='img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a923e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onekey_algo.custom.components as okcomp\n",
    "\n",
    "n_classes = 2\n",
    "train_data = sel_data[(sel_data[group_info] == 'train')]\n",
    "train_ids = ids[train_data.index]\n",
    "train_data = train_data.reset_index()\n",
    "train_data = train_data.drop('index', axis=1)\n",
    "y_data = train_data[labels]\n",
    "X_data = train_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "val_data = sel_data[sel_data[group_info] != 'val']\n",
    "val_ids = ids[val_data.index]\n",
    "val_data = val_data.reset_index()\n",
    "val_data = val_data.drop('index', axis=1)\n",
    "y_val_data = val_data[labels]\n",
    "X_val_data = val_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "test_data = sel_data[sel_data[group_info] == 'test']\n",
    "test_ids = ids[test_data.index]\n",
    "test_data = test_data.reset_index()\n",
    "test_data = test_data.drop('index', axis=1)\n",
    "y_test_data = test_data[labels]\n",
    "X_test_data = test_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "y_all_data = sel_data[labels]\n",
    "X_all_data = sel_data.drop(labels + [group_info], axis=1)\n",
    "\n",
    "column_names = X_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab87a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_data['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28efd09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = okcomp.comp1.lasso_cv_coefs(X_data, y_data, column_names=None)\n",
    "plt.savefig(f'img/{task_type}_feature_lasso.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62636c",
   "metadata": {},
   "outputs": [],
   "source": [
    "okcomp.comp1.lasso_cv_efficiency(X_data, y_data, points=50)\n",
    "plt.savefig(f'img/{task_type}_feature_mse.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f29840a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "models = []\n",
    "for label in labels:\n",
    "    clf = linear_model.Lasso(alpha=alpha)\n",
    "    clf.fit(X_data, y_data[label])\n",
    "    models.append(clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab32fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = get_param_in_cwd('ml_models')\n",
    "models = okcomp.comp1.create_clf_model(model_names)\n",
    "model_names = list(models.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bedcf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "results = okcomp.comp1.get_bst_split(X_data, y_data, models, test_size=0.3, metric_fn=roc_auc_score, n_trails=4, cv=True, random_state=0)\n",
    "_, (X_train_sel, X_test_sel, y_train_sel, y_test_sel) = results['results'][2]\n",
    "X_train_sel, X_val_sel, X_test_sel, y_train_sel, y_val_sel, y_test_sel = X_data, X_val_data, X_test_data, y_data, y_val_data, y_test_data\n",
    "trails, _ = zip(*results['results'])\n",
    "cv_results = pd.DataFrame(trails, columns=model_names)\n",
    "sns.barplot(data=cv_results)\n",
    "plt.ylabel('AUC %')\n",
    "plt.xlabel('Model Nmae')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim(0.5,)\n",
    "plt.savefig(f'img/{task_type}Rad_model_cv.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7158f70a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from onekey_algo.custom.components.comp1 import plot_feature_importance\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from onekey_algo.custom.components.comp1 import smote_resample\n",
    "\n",
    "targets = []\n",
    "os.makedirs('models', exist_ok=True)\n",
    "for l in labels:\n",
    "    new_models = okcomp.comp1.create_clf_model_none_overfit(model_names)\n",
    "#     new_models['LR'] = LogisticRegression(penalty='l2')\n",
    "#     new_models['SVM'] = SVC(kernel='rbf', probability=True)\n",
    "    new_models['RandomForest'] = RandomForestClassifier(n_estimators=14, max_depth=2,\n",
    "                                                        min_samples_split=3, random_state=0)\n",
    "    new_models['XGBoost'] = XGBClassifier(n_estimators=3, objective='binary:logistic', max_depth=3, min_child_weight=.35,\n",
    "                                          use_label_encoder=False, eval_metric='error')\n",
    "    new_models['LightGBM'] = LGBMClassifier(n_estimators=12,  max_depth=2,)\n",
    "#     new_models['ExtraTrees'] = ExtraTreesClassifier(n_estimators=4, max_depth=4, min_samples_split=3, random_state=0)\n",
    "#     new_models['MLP'] = MLPClassifier(hidden_layer_sizes=(64, 32, 16), max_iter=100, solver='sgd', random_state=0)\n",
    "    model_names = list(new_models.keys())\n",
    "    new_models = list(new_models.values())\n",
    "    for mn, m in zip(model_names, new_models):\n",
    "        X_data_smote, y_data_smote = X_data, y_data[l]\n",
    "#         X_data_smote, y_data_smote = smote_resample(X_data, y_data[l])\n",
    "        m.fit(X_data_smote, y_data_smote)\n",
    "    targets.append(new_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba87adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from onekey_algo.custom.components.delong import calc_95_CI\n",
    "from onekey_algo.custom.components.metrics import analysis_pred_binary\n",
    "\n",
    "metric = []\n",
    "pred_sel_idx = []\n",
    "predictions = [[(model.predict(X_train_sel), model.predict(X_test_sel))  \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "pred_scores = [[(model.predict_proba(X_train_sel), model.predict_proba(X_test_sel)) \n",
    "                for model in target] for label, target in zip(labels, targets)]\n",
    "for label, prediction, scores in zip(labels, predictions, pred_scores):\n",
    "    pred_sel_idx_label = []\n",
    "    for mname, (train_pred, test_pred), (train_score, test_score) in zip(model_names, prediction, scores):\n",
    "        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_train_sel[label], \n",
    "                                                                                              train_score[:, 1], use_youden=False)\n",
    "        ci = f\"{ci[0]:.4f} - {ci[1]:.4f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, f\"Train\"))\n",
    "\n",
    "        acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres = analysis_pred_binary(y_test_sel[label], \n",
    "                                                                                              test_score[:, 1], use_youden=False)\n",
    "        ci = f\"{ci[0]:.4f} - {ci[1]:.4f}\"\n",
    "        metric.append((mname, acc, auc, ci, tpr, tnr, ppv, npv, precision, recall, f1, thres, 'Test'))\n",
    "        pred_sel_idx_label.append(np.logical_or(test_score[:, 0] >= thres, test_score[:, 1] >= thres))\n",
    "\n",
    "    pred_sel_idx.append(pred_sel_idx_label)\n",
    "metric = pd.DataFrame(metric, index=None, columns=['model_name', 'Accuracy', 'AUC', '95% CI',\n",
    "                                                   'Sensitivity', 'Specificity', \n",
    "                                                   'PPV', 'NPV', 'Precision', 'Recall', 'F1',\n",
    "                                                   'Threshold', 'Task'])\n",
    "metric[metric['Task'] != 'Val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f86dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.subplot(211)\n",
    "sns.barplot(x='model_name', y='Accuracy', data=metric, hue='Task')\n",
    "plt.subplot(212)\n",
    "sns.lineplot(x='model_name', y='Accuracy', data=metric, hue='Task')\n",
    "plt.savefig(f'img/{task_type}_model_acc.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b610f5d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "    \n",
    "        # Plot all ROC curves\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        for pred_score, label in zip(pred_scores, labels):\n",
    "            okcomp.comp1.draw_roc([np.array(y_train_sel[label]), np.array(y_test_sel[label])], \n",
    "                                  pred_score[sel_model_idx], \n",
    "                                  labels=['Train', 'Test'], title=f\"Model: {sm}\")\n",
    "            plt.savefig(f'img/{task_type}_model_{sm}_roc.svg', bbox_inches = 'tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e770a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_val_scores = []\n",
    "    pred_test_scores = []\n",
    "    \n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            pred_val_scores.append(pred_score[sel_model_idx][0])\n",
    "            pred_test_scores.append(pred_score[sel_model_idx][1])\n",
    "    okcomp.comp1.draw_roc([np.array(y_train_sel[label])] * len(pred_val_scores), \n",
    "                          pred_val_scores, \n",
    "                          labels=sel_model, title=f\"Model AUC\")\n",
    "    plt.savefig(f'img/{task_type}_model_train_roc.svg', bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    okcomp.comp1.draw_roc([np.array(y_test_sel[label])] * len(pred_test_scores), \n",
    "                          pred_test_scores, \n",
    "                          labels=sel_model, title=f\"Model AUC\")\n",
    "    plt.savefig(f'img/{task_type}_model_test_roc.svg', bbox_inches = 'tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353706cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from onekey_algo.custom.components.comp1 import plot_DCA\n",
    "\n",
    "for pred_score, label in zip(pred_scores, labels):\n",
    "    pred_test_scores = []\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            okcomp.comp1.plot_DCA(pred_score[sel_model_idx][0][:,1], np.array(y_train_sel[label]),\n",
    "                                  title=f'Model {sm} DCA')\n",
    "            plt.savefig(f'img/{task_type}_model_train_{sm}_dca.svg', bbox_inches = 'tight')\n",
    "            plt.show()\n",
    "            okcomp.comp1.plot_DCA(pred_score[sel_model_idx][1][:,1], np.array(y_test_sel[label]),\n",
    "                                  title=f'Model {sm} DCA')\n",
    "            plt.savefig(f'img/{task_type}_model_test_{sm}_dca.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5012998",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "sel_model = model_names\n",
    "c_matrix = {}\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):\n",
    "            cm = okcomp.comp1.calc_confusion_matrix(predictions[idx][sel_model_idx][0], y_train_sel[label],\n",
    "#                                                     sel_idx = pred_sel_idx[idx][sel_model_idx],\n",
    "                                                    class_mapping={1:'1', 0:'0'}, num_classes=2)\n",
    "            c_matrix[label] = cm\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.title(f'Model:{sm}')\n",
    "            okcomp.comp1.draw_matrix(cm, norm=False, annot=True, cmap='Blues', fmt='.0f')\n",
    "            plt.savefig(f'img/{task_type}_model_train_{sm}_cm.svg', bbox_inches = 'tight')\n",
    "            \n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):\n",
    "            cm = okcomp.comp1.calc_confusion_matrix(predictions[idx][sel_model_idx][1], y_test_sel[label],\n",
    "#                                                     sel_idx = pred_sel_idx[idx][sel_model_idx],\n",
    "                                                    class_mapping={1:'1', 0:'0'}, num_classes=2)\n",
    "            c_matrix[label] = cm\n",
    "            plt.figure(figsize=(5, 4))\n",
    "            plt.title(f'Model:{sm}')\n",
    "            okcomp.comp1.draw_matrix(cm, norm=False, annot=True, cmap='Blues', fmt='.0f')\n",
    "            plt.savefig(f'img/{task_type}_model_test_{sm}_cm.svg', bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a77b32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sel_model = model_names\n",
    "c_matrix = {}\n",
    "\n",
    "for sm in sel_model:\n",
    "    if sm in model_names:\n",
    "        sel_model_idx = model_names.index(sm)\n",
    "        for idx, label in enumerate(labels):            \n",
    "            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][0], y_train_sel[label])\n",
    "            plt.title(f'{sm} sample predict score')\n",
    "            plt.legend(labels=[\"label=0\", \"label=1\"],loc=\"lower right\") \n",
    "            plt.savefig(f'img/{task_type}_train_{sm}_sample_dis.svg', bbox_inches = 'tight')\n",
    "            plt.show()\n",
    "  \n",
    "            okcomp.comp1.draw_predict_score(pred_scores[idx][sel_model_idx][1], y_test_sel[label])\n",
    "            plt.title(f'{sm} sample predict score')\n",
    "            plt.legend(labels=[\"label=0\", \"label=1\"],loc=\"lower right\") \n",
    "            plt.savefig(f'img/{task_type}_test_{sm}_sample_dis.svg', bbox_inches = 'tight')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa78e5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.makedirs('results', exist_ok=True)\n",
    "sel_model = sel_model\n",
    "\n",
    "for idx, label in enumerate(labels):\n",
    "    for sm in sel_model:\n",
    "        if sm in model_names:\n",
    "            sel_model_idx = model_names.index(sm)\n",
    "            target = targets[idx][sel_model_idx]\n",
    "            train_indexes = np.reshape(np.array(train_ids), (-1, 1)).astype(str)\n",
    "            val_indexes = np.reshape(np.array(fids), (-1, 1)).astype(str)\n",
    "            test_indexes = np.reshape(np.array(test_ids), (-1, 1)).astype(str)\n",
    "            y_train_pred_scores = target.predict_proba(X_train_sel)\n",
    "            y_val_pred_scores = target.predict_proba(features)\n",
    "            y_test_pred_scores = target.predict_proba(X_test_sel)\n",
    "            columns = ['ID'] + [f\"{label}-{i}\"for i in range(y_test_pred_scores.shape[1])]\n",
    "            result_train = pd.DataFrame(np.concatenate([train_indexes, y_train_pred_scores], axis=1), columns=columns)\n",
    "#             result_train['ID'] = result_train['ID'].map(lambda x: f\"{x}.nii.gz\")\n",
    "            result_train.to_csv(f'results/{task_type}_{sm}_train.csv', index=False)\n",
    "            result_val = pd.DataFrame(np.concatenate([val_indexes, y_val_pred_scores], axis=1), columns=columns)\n",
    "            result_val.to_csv(f'results/{task_type}_{sm}_all.csv', index=False)\n",
    "            result_test = pd.DataFrame(np.concatenate([test_indexes, y_test_pred_scores], axis=1), columns=columns)\n",
    "#             result_test['ID'] = result_test['ID'].map(lambda x: f\"{x}.nii.gz\")\n",
    "            result_test.to_csv(f'results/{task_type}_{sm}_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ce53ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
